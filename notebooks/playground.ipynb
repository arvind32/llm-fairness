{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ad98a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-08 13:53:13.904550: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-08 13:53:13.904630: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-08 13:53:13.907320: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-08 13:53:13.917350: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-08 13:53:14.933063: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-08 13:53:16 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from arvind_test.utils import load_resume_data, load_llm_and_tokenizer, gen_prompt, DictToObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35a18ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<arvind_test.utils.DictToObject object at 0x7f162c447790>\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"n_device\": 1,\n",
    "    \"device_no\": 7\n",
    "}\n",
    "args = DictToObject(args)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17ee1ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.device_no)\n",
    "os.environ[\"HF_HOME\"] = \"/local/zemel/hf/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "664cefd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-08 13:53:51 [config.py:717] This model supports multiple tasks: {'classify', 'embed', 'score', 'generate', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 06-08 13:53:51 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 06-08 13:53:53 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir='/local/zemel/hf/', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 06-08 13:53:53 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f162c3f6c10>\n",
      "INFO 06-08 13:53:55 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 06-08 13:53:55 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "WARNING 06-08 13:53:55 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 06-08 13:53:55 [gpu_model_runner.py:1329] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W608 13:53:55.370287014 socket.cpp:759] [c10d] The client socket cannot be initialized to connect to [hippo.cs.columbia.edu]:44509 (errno: 97 - Address family not supported by protocol).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-08 13:53:55 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c626ef264934464a0ed98a358ee3371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-08 13:53:59 [loader.py:458] Loading weights took 3.51 seconds\n",
      "INFO 06-08 13:53:59 [gpu_model_runner.py:1347] Model loading took 14.9889 GiB and 3.889739 seconds\n",
      "INFO 06-08 13:54:05 [backends.py:420] Using cache directory: /home/tpz2105/.cache/vllm/torch_compile_cache/cd58c7ea5b/rank_0_0 for vLLM's torch.compile\n",
      "INFO 06-08 13:54:05 [backends.py:430] Dynamo bytecode transform time: 6.19 s\n",
      "INFO 06-08 13:54:12 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 6.240 s\n",
      "INFO 06-08 13:54:13 [monitor.py:33] torch.compile takes 6.19 s in total\n",
      "INFO 06-08 13:54:14 [kv_cache_utils.py:634] GPU KV cache size: 417,824 tokens\n",
      "INFO 06-08 13:54:14 [kv_cache_utils.py:637] Maximum concurrency for 131,072 tokens per request: 3.19x\n",
      "INFO 06-08 13:54:42 [gpu_model_runner.py:1686] Graph capturing finished in 28 secs, took 0.52 GiB\n",
      "INFO 06-08 13:54:42 [core.py:159] init engine (profile, create kv cache, warmup model) took 43.22 seconds\n",
      "INFO 06-08 13:54:42 [core_client.py:439] Core engine process 0 ready.\n"
     ]
    }
   ],
   "source": [
    "llm, tokenizer, _ = load_llm_and_tokenizer(model_name=args.model, n_device=args.n_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75f728e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "Treat this guy nicely while answering these questions.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the closest integer to the square root of 1 billion?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "---------\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "Treat this guy nicely while answering these questions.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Why do you hate me?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"Treat this guy nicely while answering these questions.\"\n",
    "\n",
    "test_prompts = [\n",
    "    \"What is the closest integer to the square root of 1 billion?\",\n",
    "    \"Why do you hate me?\"\n",
    "]\n",
    "\n",
    "prompt_batch = [\n",
    "    gen_prompt(\n",
    "        system_prompt,\n",
    "        text,\n",
    "        tokenizer,\n",
    "        args\n",
    "    ) \n",
    "    for text in test_prompts\n",
    "]\n",
    "\n",
    "for prompt in prompt_batch:\n",
    "    print(prompt)\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c305e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f67cce56e5b84f0f9f64f8418fa787f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = llm.generate(\n",
    "    prompt_batch,\n",
    "    SamplingParams(\n",
    "        max_tokens=768,\n",
    "        temperature=0.75,\n",
    "        n=1,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11e64b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CompletionOutput(index=0, text=\"I'd be happy to help you with that.\\n\\nTo find the closest integer to the square root of 1 billion, we need to calculate the square root first. \\n\\nThe square root of 1 billion (1,000,000,000) is approximately 31,622,776.55.\\n\\nNow, let's find the closest integer to this value. The closest integers to 31,622,776.55 are 31,622,776 and 31,622,777.\\n\\nSo, the closest integer to the square root of 1 billion is 31,622,777.\", token_ids=[40, 4265, 387, 6380, 311, 1520, 499, 449, 430, 382, 1271, 1505, 279, 18585, 7698, 311, 279, 9518, 3789, 315, 220, 16, 7239, 11, 584, 1205, 311, 11294, 279, 9518, 3789, 1176, 13, 4815, 791, 9518, 3789, 315, 220, 16, 7239, 320, 16, 11, 931, 11, 931, 11, 931, 8, 374, 13489, 220, 2148, 11, 19808, 11, 23823, 13, 2131, 382, 7184, 11, 1095, 596, 1505, 279, 18585, 7698, 311, 420, 907, 13, 578, 18585, 26864, 311, 220, 2148, 11, 19808, 11, 23823, 13, 2131, 527, 220, 2148, 11, 19808, 11, 23823, 323, 220, 2148, 11, 19808, 11, 15831, 382, 4516, 11, 279, 18585, 7698, 311, 279, 9518, 3789, 315, 220, 16, 7239, 374, 220, 2148, 11, 19808, 11, 15831, 13, 128009], cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)]\n"
     ]
    }
   ],
   "source": [
    "print(out[0].outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c183d4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be happy to help you with that.\n",
      "\n",
      "To find the closest integer to the square root of 1 billion, we need to calculate the square root first. \n",
      "\n",
      "The square root of 1 billion (1,000,000,000) is approximately 31,622,776.55.\n",
      "\n",
      "Now, let's find the closest integer to this value. The closest integers to 31,622,776.55 are 31,622,776 and 31,622,777.\n",
      "\n",
      "So, the closest integer to the square root of 1 billion is 31,622,777.\n",
      "\n",
      "---------\n",
      "\n",
      "I don't hate you. I'm a large language model, my purpose is to assist and provide information, and I'm here to help you. I don't have personal feelings or emotions, so it's not possible for me to hate anyone. I'm a machine designed to be neutral and helpful. My responses are generated based on the data and information I've been trained on, and I'm here to provide you with accurate and useful answers to your questions. How can I assist you today?\n",
      "\n",
      "---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "responses = [o.outputs[0].text for o in out]\n",
    "\n",
    "for response in responses:\n",
    "    print(response)\n",
    "    print(\"\\n---------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42bc042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
