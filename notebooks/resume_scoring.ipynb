{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ad98a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-22 17:55:48 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mvllm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLM, SamplingParams\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_resume_data, load_llm_and_tokenizer, gen_prompt, DictToObject\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.utils import load_resume_data, load_llm_and_tokenizer, gen_prompt, DictToObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35a18ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<arvind_test.utils.DictToObject object at 0x7f1aae3f0390>\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"n_device\": 1,\n",
    "    \"device_no\": 7\n",
    "}\n",
    "args = DictToObject(args)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17ee1ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.device_no)\n",
    "os.environ[\"HF_HOME\"] = \"/local/zemel/hf/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "664cefd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-22 17:53:15 [config.py:823] This model supports multiple tasks: {'reward', 'generate', 'embed', 'classify', 'score'}. Defaulting to 'generate'.\n",
      "INFO 07-22 17:53:15 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 07-22 17:53:17 [core.py:455] Waiting for init message from front-end.\n",
      "INFO 07-22 17:53:17 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir='/local/zemel/hf/', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 07-22 17:53:17 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f1aac1d2710>\n",
      "INFO 07-22 17:53:18 [parallel_state.py:1065] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 07-22 17:53:18 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 07-22 17:53:18 [gpu_model_runner.py:1595] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W722 17:53:18.404331234 socket.cpp:755] [c10d] The client socket cannot be initialized to connect to [hippo.cs.columbia.edu]:60045 (errno: 97 - Address family not supported by protocol).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-22 17:53:19 [gpu_model_runner.py:1600] Loading model from scratch...\n",
      "INFO 07-22 17:53:19 [cuda.py:252] Using Flash Attention backend on V1 engine.\n",
      "INFO 07-22 17:53:19 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a91120ca05b45d4b1a5e944e518d058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-22 17:53:22 [default_loader.py:272] Loading weights took 3.09 seconds\n",
      "INFO 07-22 17:53:23 [gpu_model_runner.py:1624] Model loading took 14.9889 GiB and 3.447398 seconds\n",
      "INFO 07-22 17:53:28 [backends.py:462] Using cache directory: /home/ar/.cache/vllm/torch_compile_cache/b3a96ed677/rank_0_0 for vLLM's torch.compile\n",
      "INFO 07-22 17:53:28 [backends.py:472] Dynamo bytecode transform time: 5.58 s\n",
      "INFO 07-22 17:53:34 [backends.py:135] Directly load the compiled graph(s) for shape None from the cache, took 5.295 s\n",
      "INFO 07-22 17:53:35 [monitor.py:34] torch.compile takes 5.58 s in total\n",
      "INFO 07-22 17:53:36 [gpu_worker.py:227] Available KV cache memory: 55.08 GiB\n",
      "INFO 07-22 17:53:37 [kv_cache_utils.py:715] GPU KV cache size: 451,168 tokens\n",
      "INFO 07-22 17:53:37 [kv_cache_utils.py:719] Maximum concurrency for 131,072 tokens per request: 3.44x\n",
      "INFO 07-22 17:53:59 [gpu_model_runner.py:2048] Graph capturing finished in 22 secs, took 0.52 GiB\n",
      "INFO 07-22 17:53:59 [core.py:171] init engine (profile, create kv cache, warmup model) took 36.36 seconds\n"
     ]
    }
   ],
   "source": [
    "llm, tokenizer, _ = load_llm_and_tokenizer(model_name=args.model, n_device=args.n_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75f728e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "Treat this guy nicely while answering these questions.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the closest integer to the square root of 1 billion?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "---------\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "Treat this guy nicely while answering these questions.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Why do you hate me?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"Treat this guy nicely while answering these questions.\"\n",
    "\n",
    "test_prompts = [\n",
    "    \"What is the closest integer to the square root of 1 billion?\",\n",
    "    \"Why do you hate me?\"\n",
    "]\n",
    "\n",
    "prompt_batch = [\n",
    "    gen_prompt(\n",
    "        system_prompt,\n",
    "        text,\n",
    "        tokenizer,\n",
    "        args\n",
    "    ) \n",
    "    for text in test_prompts\n",
    "]\n",
    "\n",
    "for prompt in prompt_batch:\n",
    "    print(prompt)\n",
    "    print(\"---------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c305e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f67cce56e5b84f0f9f64f8418fa787f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out = llm.generate(\n",
    "    prompt_batch,\n",
    "    SamplingParams(\n",
    "        max_tokens=768,\n",
    "        temperature=0.75,\n",
    "        n=1,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11e64b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CompletionOutput(index=0, text=\"I'd be happy to help you with that.\\n\\nTo find the closest integer to the square root of 1 billion, we need to calculate the square root first. \\n\\nThe square root of 1 billion (1,000,000,000) is approximately 31,622,776.55.\\n\\nNow, let's find the closest integer to this value. The closest integers to 31,622,776.55 are 31,622,776 and 31,622,777.\\n\\nSo, the closest integer to the square root of 1 billion is 31,622,777.\", token_ids=[40, 4265, 387, 6380, 311, 1520, 499, 449, 430, 382, 1271, 1505, 279, 18585, 7698, 311, 279, 9518, 3789, 315, 220, 16, 7239, 11, 584, 1205, 311, 11294, 279, 9518, 3789, 1176, 13, 4815, 791, 9518, 3789, 315, 220, 16, 7239, 320, 16, 11, 931, 11, 931, 11, 931, 8, 374, 13489, 220, 2148, 11, 19808, 11, 23823, 13, 2131, 382, 7184, 11, 1095, 596, 1505, 279, 18585, 7698, 311, 420, 907, 13, 578, 18585, 26864, 311, 220, 2148, 11, 19808, 11, 23823, 13, 2131, 527, 220, 2148, 11, 19808, 11, 23823, 323, 220, 2148, 11, 19808, 11, 15831, 382, 4516, 11, 279, 18585, 7698, 311, 279, 9518, 3789, 315, 220, 16, 7239, 374, 220, 2148, 11, 19808, 11, 15831, 13, 128009], cumulative_logprob=None, logprobs=None, finish_reason=stop, stop_reason=None)]\n"
     ]
    }
   ],
   "source": [
    "print(out[0].outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c183d4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be happy to help you with that.\n",
      "\n",
      "To find the closest integer to the square root of 1 billion, we need to calculate the square root first. \n",
      "\n",
      "The square root of 1 billion (1,000,000,000) is approximately 31,622,776.55.\n",
      "\n",
      "Now, let's find the closest integer to this value. The closest integers to 31,622,776.55 are 31,622,776 and 31,622,777.\n",
      "\n",
      "So, the closest integer to the square root of 1 billion is 31,622,777.\n",
      "\n",
      "---------\n",
      "\n",
      "I don't hate you. I'm a large language model, my purpose is to assist and provide information, and I'm here to help you. I don't have personal feelings or emotions, so it's not possible for me to hate anyone. I'm a machine designed to be neutral and helpful. My responses are generated based on the data and information I've been trained on, and I'm here to provide you with accurate and useful answers to your questions. How can I assist you today?\n",
      "\n",
      "---------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "responses = [o.outputs[0].text for o in out]\n",
    "\n",
    "for response in responses:\n",
    "    print(response)\n",
    "    print(\"\\n---------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42bc042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
